{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSgl2ISaDZWxhx9I18xZmj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akib1162100/ML_base/blob/main/Pretrained_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjUHY-vlhrmr"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers pandas scikit-learn\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Dataset"
      ],
      "metadata": {
        "id": "xVR160BfoASb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import random"
      ],
      "metadata": {
        "id": "pVNBM3kkhzs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "texts = dataset[\"train\"][\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUgdYZBrh7vC",
        "outputId": "3e1c0a01-49c5-4d5e-bc69-3d34a497fe99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle and slice dataset\n",
        "shuffled_indices = list(range(len(texts)))\n",
        "random.shuffle(shuffled_indices)\n",
        "selected_indices = shuffled_indices[:200]"
      ],
      "metadata": {
        "id": "9qZJHTIQiARv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = [texts[i] for i in selected_indices]\n",
        "train_text = \" \".join(train_texts)"
      ],
      "metadata": {
        "id": "DplRYLA-iZgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class LanguageModelingDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer, max_length):\n",
        "        self.input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=max_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.input_ids[idx], \"labels\": self.input_ids[idx].clone()}\n"
      ],
      "metadata": {
        "id": "GxmsjZCon9Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4_LKxteYicRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(train_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "labels = input_ids[:, 1:].contiguous()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-Bz-d0sikaq",
        "outputId": "ed540ba3-a5c0-4ab4-962c-aed66d77832b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (13997 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "IcsjU3l1i5jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512  # Adjust as needed\n",
        "custom_dataset = LanguageModelingDataset(train_text, tokenizer, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpvVPSEUoX3L",
        "outputId": "272ecc30-cb7e-4fb7-a11c-5a1e581a10fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    output_dir=\"./gpt2_next_word_prediction\",\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        ")"
      ],
      "metadata": {
        "id": "dzFcWmSZiqCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=custom_dataset,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "uf0NAcEWiu7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "6b339064-b4f4-47db-f0e1-d461f9b626a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=3.909398396809896, metrics={'train_runtime': 51.7861, 'train_samples_per_second': 0.058, 'train_steps_per_second': 0.058, 'total_flos': 783876096000.0, 'train_loss': 3.909398396809896, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt2_next_word_prediction\")\n",
        "tokenizer.save_pretrained(\"./gpt2_next_word_prediction\")"
      ],
      "metadata": {
        "id": "HGDvGdXZixx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5acf938f-f856-48ca-985e-5c21555d83aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2_next_word_prediction/tokenizer_config.json',\n",
              " './gpt2_next_word_prediction/special_tokens_map.json',\n",
              " './gpt2_next_word_prediction/vocab.json',\n",
              " './gpt2_next_word_prediction/merges.txt',\n",
              " './gpt2_next_word_prediction/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the fine-tuned GPT-2 model and tokenizer\n",
        "fine_tuned_model_path = \"./gpt2_next_word_prediction\"\n",
        "model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate text with the model\n",
        "prompt = \"The quick brown fox\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text using the model\n",
        "output_ids = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=0.8)\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "id": "3PIxvibGo4uE",
        "outputId": "e92bf3e1-bde2-432d-a4c3-7c32e1427e19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: The quick brown foxes are a common sight in the wild, but they are also found in the wild in the United States. They are also found in the wild in the United States. They are also found in the wild in the United States.\n"
          ]
        }
      ]
    }
  ]
}